{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.7 64-bit ('tf-gpu': conda)",
   "metadata": {
    "interpreter": {
     "hash": "547696d57ba429e318c25679796690da565e30c488d1ba2b908ff43b52e35683"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "#  Deep Reinforcement Learning for Automated Stock Trading Ensemble Strategy ICAIF 2020 Notebook_version"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "This will be the single notebook to represent the all results of the system\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Importing Libraries "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gym.utils import seeding\n",
    "import gym\n",
    "from gym import spaces\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from stockstats import StockDataFrame as Sdf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DATA_FILE = \"data/dow_30_2009_2020.csv\"\n",
    "TURBULENCE_DATA = \"data/dow30_turbulence_index.csv\"\n",
    "TESTING_DATA_FILE = \"test.csv\"\n",
    "now = datetime.datetime.now()\n",
    "TRAINED_MODEL_DIR = \"trained_models\"\n",
    "os.makedirs(TRAINED_MODEL_DIR)"
   ]
  },
  {
   "source": [
    "## Environments"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Defining initial parameters for all the environments"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shares normalization factor\n",
    "# 100 shares per trade\n",
    "HMAX_NORMALIZE = 100\n",
    "# initial amount of money we have in our account\n",
    "INITIAL_ACCOUNT_BALANCE=1000000\n",
    "# total number of stocks in our portfolio\n",
    "STOCK_DIM = 30\n",
    "# transaction fee: 1/1000 reasonable percentage\n",
    "TRANSACTION_FEE_PERCENT = 0.001\n",
    "\n",
    "# turbulence index: 90-150 reasonable threshold\n",
    "#TURBULENCE_THRESHOLD = 140 eg\n",
    "REWARD_SCALING = 1e-4"
   ]
  },
  {
   "source": [
    "This is the function environment for stock trading"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockEnvTrade(gym.Env):\n",
    "    \"\"\"A stock trading environment for OpenAI gym\"\"\"\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, df,day = 0,turbulence_threshold=140\n",
    "                 ,initial=True, previous_state=[], model_name='', iteration=''):\n",
    "        #super(StockEnv, self).__init__()\n",
    "        #money = 10 , scope = 1\n",
    "        self.day = day\n",
    "        self.df = df\n",
    "        self.initial = initial\n",
    "        self.previous_state = previous_state\n",
    "        # action_space normalization and shape is STOCK_DIM\n",
    "        self.action_space = spaces.Box(low = -1, high = 1,shape = (STOCK_DIM,)) \n",
    "        self.observation_space = spaces.Box(low=0, high=np.inf, shape = (181,))\n",
    "        # load data from a pandas dataframe\n",
    "        self.data = self.df.loc[self.day,:]\n",
    "        self.terminal = False     \n",
    "        self.turbulence_threshold = turbulence_threshold\n",
    "        # initalize state\n",
    "        self.state = [INITIAL_ACCOUNT_BALANCE] + \\\n",
    "                      self.data.adjcp.values.tolist() + \\\n",
    "                      [0]*STOCK_DIM + \\\n",
    "                      self.data.macd.values.tolist() + \\\n",
    "                      self.data.rsi.values.tolist() + \\\n",
    "                      self.data.cci.values.tolist() + \\\n",
    "                      self.data.adx.values.tolist()\n",
    "        # initialize reward\n",
    "        self.reward = 0\n",
    "        self.turbulence = 0\n",
    "        self.cost = 0\n",
    "        self.trades = 0\n",
    "        # memorize all the total balance change\n",
    "        self.asset_memory = [INITIAL_ACCOUNT_BALANCE]\n",
    "        self.rewards_memory = []\n",
    "        #self.reset()\n",
    "        self._seed()\n",
    "        self.model_name=model_name        \n",
    "        self.iteration=iteration\n",
    "\n",
    "\n",
    "    def _sell_stock(self, index, action):\n",
    "        # perform sell action based on the sign of the action\n",
    "        if self.turbulence<self.turbulence_threshold:\n",
    "            if self.state[index+STOCK_DIM+1] > 0:\n",
    "                #update balance\n",
    "                self.state[0] += \\\n",
    "                self.state[index+1]*min(abs(action),self.state[index+STOCK_DIM+1]) * \\\n",
    "                 (1- TRANSACTION_FEE_PERCENT)\n",
    "                \n",
    "                self.state[index+STOCK_DIM+1] -= min(abs(action), self.state[index+STOCK_DIM+1])\n",
    "                self.cost +=self.state[index+1]*min(abs(action),self.state[index+STOCK_DIM+1]) * \\\n",
    "                 TRANSACTION_FEE_PERCENT\n",
    "                self.trades+=1\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            # if turbulence goes over threshold, just clear out all positions \n",
    "            if self.state[index+STOCK_DIM+1] > 0:\n",
    "                #update balance\n",
    "                self.state[0] += self.state[index+1]*self.state[index+STOCK_DIM+1]* \\\n",
    "                              (1- TRANSACTION_FEE_PERCENT)\n",
    "                self.state[index+STOCK_DIM+1] =0\n",
    "                self.cost += self.state[index+1]*self.state[index+STOCK_DIM+1]* \\\n",
    "                              TRANSACTION_FEE_PERCENT\n",
    "                self.trades+=1\n",
    "            else:\n",
    "                pass\n",
    "    \n",
    "    def _buy_stock(self, index, action):\n",
    "        # perform buy action based on the sign of the action\n",
    "        if self.turbulence< self.turbulence_threshold:\n",
    "            available_amount = self.state[0] // self.state[index+1]\n",
    "            # print('available_amount:{}'.format(available_amount))\n",
    "            \n",
    "            #update balance\n",
    "            self.state[0] -= self.state[index+1]*min(available_amount, action)* \\\n",
    "                              (1+ TRANSACTION_FEE_PERCENT)\n",
    "\n",
    "            self.state[index+STOCK_DIM+1] += min(available_amount, action)\n",
    "            \n",
    "            self.cost+=self.state[index+1]*min(available_amount, action)* \\\n",
    "                              TRANSACTION_FEE_PERCENT\n",
    "            self.trades+=1\n",
    "        else:\n",
    "            # if turbulence goes over threshold, just stop buying\n",
    "            pass\n",
    "        \n",
    "    def step(self, actions):\n",
    "        self.terminal = self.day >= len(self.df.index.unique())-1\n",
    "\n",
    "        if self.terminal:\n",
    "            plt.plot(self.asset_memory,'r')\n",
    "            plt.savefig('results/account_value_trade_{}_{}.png'.format(self.model_name, self.iteration))\n",
    "            plt.close()\n",
    "            df_total_value = pd.DataFrame(self.asset_memory)\n",
    "            df_total_value.to_csv('results/account_value_trade_{}_{}.csv'.format(self.model_name, self.iteration))\n",
    "            end_total_asset = self.state[0]+ \\\n",
    "            sum(np.array(self.state[1:(STOCK_DIM+1)])*np.array(self.state[(STOCK_DIM+1):(STOCK_DIM*2+1)]))\n",
    "            print(\"previous_total_asset:{}\".format(self.asset_memory[0]))           \n",
    "\n",
    "            print(\"end_total_asset:{}\".format(end_total_asset))\n",
    "            print(\"total_reward:{}\".format(self.state[0]+sum(np.array(self.state[1:(STOCK_DIM+1)])*np.array(self.state[(STOCK_DIM+1):(STOCK_DIM*2+1)]))- self.asset_memory[0] ))\n",
    "            print(\"total_cost: \", self.cost)\n",
    "            print(\"total trades: \", self.trades)\n",
    "\n",
    "            df_total_value.columns = ['account_value']\n",
    "            df_total_value['daily_return']=df_total_value.pct_change(1)\n",
    "            sharpe = (4**0.5)*df_total_value['daily_return'].mean()/ \\\n",
    "                  df_total_value['daily_return'].std()\n",
    "            print(\"Sharpe: \",sharpe)\n",
    "            \n",
    "            df_rewards = pd.DataFrame(self.rewards_memory)\n",
    "            df_rewards.to_csv('results/account_rewards_trade_{}_{}.csv'.format(self.model_name, self.iteration))\n",
    "            \n",
    "            \n",
    "            return self.state, self.reward, self.terminal,{}\n",
    "\n",
    "        else:\n",
    "            \n",
    "            actions = actions * HMAX_NORMALIZE\n",
    "            \n",
    "            if self.turbulence>=self.turbulence_threshold:\n",
    "                actions=np.array([-HMAX_NORMALIZE]*STOCK_DIM)\n",
    "                \n",
    "            begin_total_asset = self.state[0]+ \\\n",
    "            sum(np.array(self.state[1:(STOCK_DIM+1)])*np.array(self.state[(STOCK_DIM+1):(STOCK_DIM*2+1)]))\n",
    "            #print(\"begin_total_asset:{}\".format(begin_total_asset))\n",
    "            \n",
    "            argsort_actions = np.argsort(actions)\n",
    "            \n",
    "            sell_index = argsort_actions[:np.where(actions < 0)[0].shape[0]]\n",
    "            buy_index = argsort_actions[::-1][:np.where(actions > 0)[0].shape[0]]\n",
    "\n",
    "            for index in sell_index:\n",
    "                self._sell_stock(index, actions[index])\n",
    "\n",
    "            for index in buy_index:\n",
    "                self._buy_stock(index, actions[index])\n",
    "\n",
    "            self.day += 1\n",
    "            self.data = self.df.loc[self.day,:]         \n",
    "            self.turbulence = self.data['turbulence'].values[0]\n",
    "            self.state =  [self.state[0]] + \\\n",
    "                    self.data.adjcp.values.tolist() + \\\n",
    "                    list(self.state[(STOCK_DIM+1):(STOCK_DIM*2+1)]) + \\\n",
    "                    self.data.macd.values.tolist() + \\\n",
    "                    self.data.rsi.values.tolist() + \\\n",
    "                    self.data.cci.values.tolist() + \\\n",
    "                    self.data.adx.values.tolist()\n",
    "            \n",
    "            end_total_asset = self.state[0]+ \\\n",
    "            sum(np.array(self.state[1:(STOCK_DIM+1)])*np.array(self.state[(STOCK_DIM+1):(STOCK_DIM*2+1)]))\n",
    "            self.asset_memory.append(end_total_asset)\n",
    "            \n",
    "            self.reward = end_total_asset - begin_total_asset            \n",
    "            self.rewards_memory.append(self.reward)\n",
    "            \n",
    "            self.reward = self.reward*REWARD_SCALING\n",
    "\n",
    "\n",
    "        return self.state, self.reward, self.terminal, {}\n",
    "\n",
    "    def reset(self):  \n",
    "        if self.initial:\n",
    "            self.asset_memory = [INITIAL_ACCOUNT_BALANCE]\n",
    "            self.day = 0\n",
    "            self.data = self.df.loc[self.day,:]\n",
    "            self.turbulence = 0\n",
    "            self.cost = 0\n",
    "            self.trades = 0\n",
    "            self.terminal = False \n",
    "            self.rewards_memory = []\n",
    "            #initiate state\n",
    "            self.state = [INITIAL_ACCOUNT_BALANCE] + \\\n",
    "                          self.data.adjcp.values.tolist() + \\\n",
    "                          [0]*STOCK_DIM + \\\n",
    "                          self.data.macd.values.tolist() + \\\n",
    "                          self.data.rsi.values.tolist()  + \\\n",
    "                          self.data.cci.values.tolist()  + \\\n",
    "                          self.data.adx.values.tolist() \n",
    "        else:\n",
    "            previous_total_asset = self.previous_state[0]+ \\\n",
    "            sum(np.array(self.previous_state[1:(STOCK_DIM+1)])*np.array(self.previous_state[(STOCK_DIM+1):(STOCK_DIM*2+1)]))\n",
    "            self.asset_memory = [previous_total_asset]\n",
    "            self.day = 0\n",
    "            self.data = self.df.loc[self.day,:]\n",
    "            self.turbulence = 0\n",
    "            self.cost = 0\n",
    "            self.trades = 0\n",
    "            self.terminal = False \n",
    "            self.rewards_memory = []\n",
    "\n",
    "            self.state = [ self.previous_state[0]] + \\\n",
    "                          self.data.adjcp.values.tolist() + \\\n",
    "                          self.previous_state[(STOCK_DIM+1):(STOCK_DIM*2+1)]+ \\\n",
    "                          self.data.macd.values.tolist() + \\\n",
    "                          self.data.rsi.values.tolist()  + \\\n",
    "                          self.data.cci.values.tolist()  + \\\n",
    "                          self.data.adx.values.tolist() \n",
    "            \n",
    "        return self.state\n",
    "    \n",
    "    def render(self, mode='human',close=False):\n",
    "        return self.state\n",
    "    \n",
    "\n",
    "    def _seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]"
   ]
  },
  {
   "source": [
    "Environment Multiple Stock for training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockEnvTrain(gym.Env):\n",
    "    \"\"\"A stock trading environment for OpenAI gym\"\"\"\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, df,day = 0):\n",
    "        self.day = day\n",
    "        self.df = df\n",
    "\n",
    "        # action_space normalization and shape is STOCK_DIM\n",
    "        self.action_space = spaces.Box(low = -1, high = 1,shape = (STOCK_DIM,)) \n",
    "        self.observation_space = spaces.Box(low=0, high=np.inf, shape = (181,))\n",
    "        # load data from a pandas dataframe\n",
    "        self.data = self.df.loc[self.day,:]\n",
    "        self.terminal = False             \n",
    "        # initalize state\n",
    "        self.state = [INITIAL_ACCOUNT_BALANCE] + \\\n",
    "                      self.data.adjcp.values.tolist() + \\\n",
    "                      [0]*STOCK_DIM + \\\n",
    "                      self.data.macd.values.tolist() + \\\n",
    "                      self.data.rsi.values.tolist() + \\\n",
    "                      self.data.cci.values.tolist() + \\\n",
    "                      self.data.adx.values.tolist()\n",
    "        # initialize reward\n",
    "        self.reward = 0\n",
    "        self.cost = 0\n",
    "        # memorize all the total balance change\n",
    "        self.asset_memory = [INITIAL_ACCOUNT_BALANCE]\n",
    "        self.rewards_memory = []\n",
    "        self.trades = 0\n",
    "        self._seed()\n",
    "\n",
    "\n",
    "    def _sell_stock(self, index, action):\n",
    "        # perform sell action based on the sign of the action\n",
    "        if self.state[index+STOCK_DIM+1] > 0:\n",
    "            #update balance\n",
    "            self.state[0] += \\\n",
    "            self.state[index+1]*min(abs(action),self.state[index+STOCK_DIM+1]) * \\\n",
    "             (1- TRANSACTION_FEE_PERCENT)\n",
    "\n",
    "            self.state[index+STOCK_DIM+1] -= min(abs(action), self.state[index+STOCK_DIM+1])\n",
    "            self.cost +=self.state[index+1]*min(abs(action),self.state[index+STOCK_DIM+1]) * \\\n",
    "             TRANSACTION_FEE_PERCENT\n",
    "            self.trades+=1\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    \n",
    "    def _buy_stock(self, index, action):\n",
    "        # perform buy action based on the sign of the action\n",
    "        available_amount = self.state[0] // self.state[index+1]\n",
    "\n",
    "        #update balance\n",
    "        self.state[0] -= self.state[index+1]*min(available_amount, action)* \\\n",
    "                          (1+ TRANSACTION_FEE_PERCENT)\n",
    "\n",
    "        self.state[index+STOCK_DIM+1] += min(available_amount, action)\n",
    "\n",
    "        self.cost+=self.state[index+1]*min(available_amount, action)* \\\n",
    "                          TRANSACTION_FEE_PERCENT\n",
    "        self.trades+=1\n",
    "        \n",
    "    def step(self, actions):\n",
    "        self.terminal = self.day >= len(self.df.index.unique())-1\n",
    "\n",
    "        if self.terminal:\n",
    "            plt.plot(self.asset_memory,'r')\n",
    "            plt.savefig('results/account_value_train.png')\n",
    "            plt.close()\n",
    "            end_total_asset = self.state[0]+ \\\n",
    "            sum(np.array(self.state[1:(STOCK_DIM+1)])*np.array(self.state[(STOCK_DIM+1):(STOCK_DIM*2+1)]))\n",
    "            \n",
    "            df_total_value = pd.DataFrame(self.asset_memory)\n",
    "            df_total_value.to_csv('results/account_value_train.csv')\n",
    "            df_total_value.columns = ['account_value']\n",
    "            df_total_value['daily_return']=df_total_value.pct_change(1)\n",
    "            sharpe = (252**0.5)*df_total_value['daily_return'].mean()/ \\\n",
    "                  df_total_value['daily_return'].std()\n",
    "            df_rewards = pd.DataFrame(self.rewards_memory)\n",
    "            \n",
    "            \n",
    "            return self.state, self.reward, self.terminal,{}\n",
    "\n",
    "        else:\n",
    "\n",
    "            actions = actions * HMAX_NORMALIZE\n",
    "            \n",
    "            begin_total_asset = self.state[0]+ \\\n",
    "            sum(np.array(self.state[1:(STOCK_DIM+1)])*np.array(self.state[(STOCK_DIM+1):(STOCK_DIM*2+1)]))\n",
    "            \n",
    "            argsort_actions = np.argsort(actions)\n",
    "            \n",
    "            sell_index = argsort_actions[:np.where(actions < 0)[0].shape[0]]\n",
    "            buy_index = argsort_actions[::-1][:np.where(actions > 0)[0].shape[0]]\n",
    "\n",
    "            for index in sell_index:\n",
    "                self._sell_stock(index, actions[index])\n",
    "\n",
    "            for index in buy_index:\n",
    "                self._buy_stock(index, actions[index])\n",
    "\n",
    "            self.day += 1\n",
    "            self.data = self.df.loc[self.day,:]         \n",
    "            #load next state\n",
    "            self.state =  [self.state[0]] + \\\n",
    "                    self.data.adjcp.values.tolist() + \\\n",
    "                    list(self.state[(STOCK_DIM+1):(STOCK_DIM*2+1)]) + \\\n",
    "                    self.data.macd.values.tolist() + \\\n",
    "                    self.data.rsi.values.tolist() + \\\n",
    "                    self.data.cci.values.tolist() + \\\n",
    "                    self.data.adx.values.tolist()\n",
    "            \n",
    "            end_total_asset = self.state[0]+ \\\n",
    "            sum(np.array(self.state[1:(STOCK_DIM+1)])*np.array(self.state[(STOCK_DIM+1):(STOCK_DIM*2+1)]))\n",
    "            self.asset_memory.append(end_total_asset)\n",
    "            \n",
    "            self.reward = end_total_asset - begin_total_asset   \n",
    "            self.rewards_memory.append(self.reward)\n",
    "            \n",
    "            self.reward = self.reward*REWARD_SCALING\n",
    "\n",
    "\n",
    "\n",
    "        return self.state, self.reward, self.terminal, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.asset_memory = [INITIAL_ACCOUNT_BALANCE]\n",
    "        self.day = 0\n",
    "        self.data = self.df.loc[self.day,:]\n",
    "        self.cost = 0\n",
    "        self.trades = 0\n",
    "        self.terminal = False \n",
    "        self.rewards_memory = []\n",
    "        #initiate state\n",
    "        self.state = [INITIAL_ACCOUNT_BALANCE] + \\\n",
    "                      self.data.adjcp.values.tolist() + \\\n",
    "                      [0]*STOCK_DIM + \\\n",
    "                      self.data.macd.values.tolist() + \\\n",
    "                      self.data.rsi.values.tolist() + \\\n",
    "                      self.data.cci.values.tolist() + \\\n",
    "                      self.data.adx.values.tolist() \n",
    "        return self.state\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        return self.state\n",
    "\n",
    "    def _seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]"
   ]
  },
  {
   "source": [
    "Environment for multiple stocks for validation testing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockEnvValidation(gym.Env):\n",
    "    \"\"\"A stock trading environment for OpenAI gym\"\"\"\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, df, day = 0, turbulence_threshold=140, iteration=''):\n",
    "        #money = 10 , scope = 1\n",
    "        self.day = day\n",
    "        self.df = df\n",
    "        # action_space normalization and shape is STOCK_DIM\n",
    "        self.action_space = spaces.Box(low = -1, high = 1,shape = (STOCK_DIM,)) \n",
    "        self.observation_space = spaces.Box(low=0, high=np.inf, shape = (181,))\n",
    "        # load data from a pandas dataframe\n",
    "        self.data = self.df.loc[self.day,:]\n",
    "        self.terminal = False     \n",
    "        self.turbulence_threshold = turbulence_threshold\n",
    "        # initalize state\n",
    "        self.state = [INITIAL_ACCOUNT_BALANCE] + \\\n",
    "                      self.data.adjcp.values.tolist() + \\\n",
    "                      [0]*STOCK_DIM + \\\n",
    "                      self.data.macd.values.tolist() + \\\n",
    "                      self.data.rsi.values.tolist() + \\\n",
    "                      self.data.cci.values.tolist() + \\\n",
    "                      self.data.adx.values.tolist()\n",
    "        # initialize reward\n",
    "        self.reward = 0\n",
    "        self.turbulence = 0\n",
    "        self.cost = 0\n",
    "        self.trades = 0\n",
    "        # memorize all the total balance change\n",
    "        self.asset_memory = [INITIAL_ACCOUNT_BALANCE]\n",
    "        self.rewards_memory = []\n",
    "        #self.reset()\n",
    "        self._seed()\n",
    "        \n",
    "        self.iteration=iteration\n",
    "\n",
    "\n",
    "    def _sell_stock(self, index, action):\n",
    "        # perform sell action based on the sign of the action\n",
    "        if self.turbulence<self.turbulence_threshold:\n",
    "            if self.state[index+STOCK_DIM+1] > 0:\n",
    "                #update balance\n",
    "                self.state[0] += \\\n",
    "                self.state[index+1]*min(abs(action),self.state[index+STOCK_DIM+1]) * \\\n",
    "                 (1- TRANSACTION_FEE_PERCENT)\n",
    "                \n",
    "                self.state[index+STOCK_DIM+1] -= min(abs(action), self.state[index+STOCK_DIM+1])\n",
    "                self.cost +=self.state[index+1]*min(abs(action),self.state[index+STOCK_DIM+1]) * \\\n",
    "                 TRANSACTION_FEE_PERCENT\n",
    "                self.trades+=1\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            # if turbulence goes over threshold, just clear out all positions \n",
    "            if self.state[index+STOCK_DIM+1] > 0:\n",
    "                #update balance\n",
    "                self.state[0] += self.state[index+1]*self.state[index+STOCK_DIM+1]* \\\n",
    "                              (1- TRANSACTION_FEE_PERCENT)\n",
    "                self.state[index+STOCK_DIM+1] =0\n",
    "                self.cost += self.state[index+1]*self.state[index+STOCK_DIM+1]* \\\n",
    "                              TRANSACTION_FEE_PERCENT\n",
    "                self.trades+=1\n",
    "            else:\n",
    "                pass\n",
    "    \n",
    "    def _buy_stock(self, index, action):\n",
    "        # perform buy action based on the sign of the action\n",
    "        if self.turbulence< self.turbulence_threshold:\n",
    "            available_amount = self.state[0] // self.state[index+1]\n",
    "            \n",
    "            #update balance\n",
    "            self.state[0] -= self.state[index+1]*min(available_amount, action)* \\\n",
    "                              (1+ TRANSACTION_FEE_PERCENT)\n",
    "\n",
    "            self.state[index+STOCK_DIM+1] += min(available_amount, action)\n",
    "            \n",
    "            self.cost+=self.state[index+1]*min(available_amount, action)* \\\n",
    "                              TRANSACTION_FEE_PERCENT\n",
    "            self.trades+=1\n",
    "        else:\n",
    "            # if turbulence goes over threshold, just stop buying\n",
    "            pass\n",
    "        \n",
    "    def step(self, actions):\n",
    "        self.terminal = self.day >= len(self.df.index.unique())-1\n",
    "\n",
    "        if self.terminal:\n",
    "            plt.plot(self.asset_memory,'r')\n",
    "            plt.savefig('results/account_value_validation_{}.png'.format(self.iteration))\n",
    "            plt.close()\n",
    "            df_total_value = pd.DataFrame(self.asset_memory)\n",
    "            df_total_value.to_csv('results/account_value_validation_{}.csv'.format(self.iteration))\n",
    "            end_total_asset = self.state[0]+ \\\n",
    "            sum(np.array(self.state[1:(STOCK_DIM+1)])*np.array(self.state[(STOCK_DIM+1):(STOCK_DIM*2+1)]))\n",
    "\n",
    "            df_total_value.columns = ['account_value']\n",
    "            df_total_value['daily_return']=df_total_value.pct_change(1)\n",
    "            sharpe = (4**0.5)*df_total_value['daily_return'].mean()/ \\\n",
    "                  df_total_value['daily_return'].std()\n",
    "            \n",
    "            \n",
    "            return self.state, self.reward, self.terminal,{}\n",
    "\n",
    "        else:\n",
    "            \n",
    "\n",
    "            actions = actions * HMAX_NORMALIZE\n",
    "            \n",
    "            if self.turbulence>=self.turbulence_threshold:\n",
    "                actions=np.array([-HMAX_NORMALIZE]*STOCK_DIM)\n",
    "            begin_total_asset = self.state[0]+ \\\n",
    "            sum(np.array(self.state[1:(STOCK_DIM+1)])*np.array(self.state[(STOCK_DIM+1):(STOCK_DIM*2+1)]))\n",
    "            \n",
    "            argsort_actions = np.argsort(actions)\n",
    "            \n",
    "            sell_index = argsort_actions[:np.where(actions < 0)[0].shape[0]]\n",
    "            buy_index = argsort_actions[::-1][:np.where(actions > 0)[0].shape[0]]\n",
    "\n",
    "            for index in sell_index:\n",
    "                \n",
    "                self._sell_stock(index, actions[index])\n",
    "\n",
    "            for index in buy_index:\n",
    "                \n",
    "                self._buy_stock(index, actions[index])\n",
    "\n",
    "            self.day += 1\n",
    "            self.data = self.df.loc[self.day,:]         \n",
    "            self.turbulence = self.data['turbulence'].values[0]\n",
    "            \n",
    "            #load next state\n",
    "            \n",
    "            self.state =  [self.state[0]] + \\\n",
    "                    self.data.adjcp.values.tolist() + \\\n",
    "                    list(self.state[(STOCK_DIM+1):(STOCK_DIM*2+1)]) + \\\n",
    "                    self.data.macd.values.tolist() + \\\n",
    "                    self.data.rsi.values.tolist() + \\\n",
    "                    self.data.cci.values.tolist() + \\\n",
    "                    self.data.adx.values.tolist()\n",
    "            \n",
    "            end_total_asset = self.state[0]+ \\\n",
    "            sum(np.array(self.state[1:(STOCK_DIM+1)])*np.array(self.state[(STOCK_DIM+1):(STOCK_DIM*2+1)]))\n",
    "            self.asset_memory.append(end_total_asset)\n",
    "            \n",
    "            self.reward = end_total_asset - begin_total_asset            \n",
    "\n",
    "            self.rewards_memory.append(self.reward)\n",
    "            \n",
    "            self.reward = self.reward*REWARD_SCALING\n",
    "\n",
    "        return self.state, self.reward, self.terminal, {}\n",
    "\n",
    "    def reset(self):  \n",
    "        self.asset_memory = [INITIAL_ACCOUNT_BALANCE]\n",
    "        self.day = 0\n",
    "        self.data = self.df.loc[self.day,:]\n",
    "        self.turbulence = 0\n",
    "        self.cost = 0\n",
    "        self.trades = 0\n",
    "        self.terminal = False \n",
    "        #self.iteration=self.iteration\n",
    "        self.rewards_memory = []\n",
    "        #initiate state\n",
    "        self.state = [INITIAL_ACCOUNT_BALANCE] + \\\n",
    "                      self.data.adjcp.values.tolist() + \\\n",
    "                      [0]*STOCK_DIM + \\\n",
    "                      self.data.macd.values.tolist() + \\\n",
    "                      self.data.rsi.values.tolist()  + \\\n",
    "                      self.data.cci.values.tolist()  + \\\n",
    "                      self.data.adx.values.tolist() \n",
    "            \n",
    "        return self.state\n",
    "    \n",
    "    def render(self, mode='human',close=False):\n",
    "        return self.state\n",
    "    \n",
    "\n",
    "    def _seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]"
   ]
  },
  {
   "source": [
    "## Pre-Processing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Creating a section for preprocessing the data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(*, file_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    load csv dataset from path\n",
    "    :return: (df) pandas dataframe\n",
    "    \"\"\"\n",
    "    _data = pd.read_csv(file_name)\n",
    "    return _data\n",
    "\n",
    "def data_split(df,start,end):\n",
    "    \"\"\"\n",
    "    split the dataset into training or testing using date\n",
    "    :param data: (df) pandas dataframe, start, end\n",
    "    :return: (df) pandas dataframe\n",
    "    \"\"\"\n",
    "    data = df[(df.datadate >= start) & (df.datadate < end)]\n",
    "    data=data.sort_values(['datadate','tic'],ignore_index=True)\n",
    "    data.index = data.datadate.factorize()[0]\n",
    "    return data\n",
    "\n",
    "def calcualte_price(df):\n",
    "    \"\"\"\n",
    "    calcualte adjusted close price, open-high-low price and volume\n",
    "    :param data: (df) pandas dataframe\n",
    "    :return: (df) pandas dataframe\n",
    "    \"\"\"\n",
    "    data = df.copy()\n",
    "    data = data[['datadate', 'tic', 'prccd', 'ajexdi', 'prcod', 'prchd', 'prcld', 'cshtrd']]\n",
    "    data['ajexdi'] = data['ajexdi'].apply(lambda x: 1 if x == 0 else x)\n",
    "\n",
    "    data['adjcp'] = data['prccd'] / data['ajexdi']\n",
    "    data['open'] = data['prcod'] / data['ajexdi']\n",
    "    data['high'] = data['prchd'] / data['ajexdi']\n",
    "    data['low'] = data['prcld'] / data['ajexdi']\n",
    "    data['volume'] = data['cshtrd']\n",
    "\n",
    "    data = data[['datadate', 'tic', 'adjcp', 'open', 'high', 'low', 'volume']]\n",
    "    data = data.sort_values(['tic', 'datadate'], ignore_index=True)\n",
    "    return data\n",
    "\n",
    "def add_technical_indicator(df):\n",
    "    \"\"\"\n",
    "    calcualte technical indicators\n",
    "    use stockstats package to add technical inidactors\n",
    "    :param data: (df) pandas dataframe\n",
    "    :return: (df) pandas dataframe\n",
    "    \"\"\"\n",
    "    stock = Sdf.retype(df.copy())\n",
    "\n",
    "    stock['close'] = stock['adjcp']\n",
    "    unique_ticker = stock.tic.unique()\n",
    "\n",
    "    macd = pd.DataFrame()\n",
    "    rsi = pd.DataFrame()\n",
    "    cci = pd.DataFrame()\n",
    "    dx = pd.DataFrame()\n",
    "\n",
    "    #temp = stock[stock.tic == unique_ticker[0]]['macd']\n",
    "    for i in range(len(unique_ticker)):\n",
    "        ## macd\n",
    "        temp_macd = stock[stock.tic == unique_ticker[i]]['macd']\n",
    "        temp_macd = pd.DataFrame(temp_macd)\n",
    "        macd = macd.append(temp_macd, ignore_index=True)\n",
    "        ## rsi\n",
    "        temp_rsi = stock[stock.tic == unique_ticker[i]]['rsi_30']\n",
    "        temp_rsi = pd.DataFrame(temp_rsi)\n",
    "        rsi = rsi.append(temp_rsi, ignore_index=True)\n",
    "        ## cci\n",
    "        temp_cci = stock[stock.tic == unique_ticker[i]]['cci_30']\n",
    "        temp_cci = pd.DataFrame(temp_cci)\n",
    "        cci = cci.append(temp_cci, ignore_index=True)\n",
    "        ## adx\n",
    "        temp_dx = stock[stock.tic == unique_ticker[i]]['dx_30']\n",
    "        temp_dx = pd.DataFrame(temp_dx)\n",
    "        dx = dx.append(temp_dx, ignore_index=True)\n",
    "\n",
    "\n",
    "    df['macd'] = macd\n",
    "    df['rsi'] = rsi\n",
    "    df['cci'] = cci\n",
    "    df['adx'] = dx\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_data():\n",
    "    \"\"\"data preprocessing pipeline\"\"\"\n",
    "\n",
    "    df = load_dataset(file_name=TRAINING_DATA_FILE)\n",
    "    # get data after 2009\n",
    "    df = df[df.datadate>=20090000]\n",
    "    # calcualte adjusted price\n",
    "    df_preprocess = calcualte_price(df)\n",
    "    # add technical indicators using stockstats\n",
    "    df_final=add_technical_indicator(df_preprocess)\n",
    "    # fill the missing values at the beginning\n",
    "    df_final.fillna(method='bfill',inplace=True)\n",
    "    return df_final\n",
    "\n",
    "def add_turbulence(df):\n",
    "    \"\"\"\n",
    "    add turbulence index from a precalcualted dataframe\n",
    "    :param data: (df) pandas dataframe\n",
    "    :return: (df) pandas dataframe\n",
    "    \"\"\"\n",
    "    turbulence_index = calcualte_turbulence(df)\n",
    "    df = df.merge(turbulence_index, on='datadate')\n",
    "    df = df.sort_values(['datadate','tic']).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def calcualte_turbulence(df):\n",
    "    \"\"\"calculate turbulence index based on dow 30\"\"\"\n",
    "    # can add other market assets\n",
    "    \n",
    "    df_price_pivot=df.pivot(index='datadate', columns='tic', values='adjcp')\n",
    "    unique_date = df.datadate.unique()\n",
    "    # start after a year\n",
    "    start = 252\n",
    "    turbulence_index = [0]*start\n",
    "    count=0\n",
    "    for i in range(start,len(unique_date)):\n",
    "        current_price = df_price_pivot[df_price_pivot.index == unique_date[i]]\n",
    "        hist_price = df_price_pivot[[n in unique_date[0:i] for n in df_price_pivot.index ]]\n",
    "        cov_temp = hist_price.cov()\n",
    "        current_temp=(current_price - np.mean(hist_price,axis=0))\n",
    "        temp = current_temp.values.dot(np.linalg.inv(cov_temp)).dot(current_temp.values.T)\n",
    "        if temp>0:\n",
    "            count+=1\n",
    "            if count>2:\n",
    "                turbulence_temp = temp[0][0]\n",
    "            else:\n",
    "                #avoid large outlier because of the calculation just begins\n",
    "                turbulence_temp=0\n",
    "        else:\n",
    "            turbulence_temp=0\n",
    "        turbulence_index.append(turbulence_temp)\n",
    "    \n",
    "    \n",
    "    turbulence_index = pd.DataFrame({'datadate':df_price_pivot.index,\n",
    "                                     'turbulence':turbulence_index})\n",
    "    return turbulence_index"
   ]
  },
  {
   "source": [
    "## Model deployment"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "In this Notebook I am using stable_baselines3 library instead of stable_baselines which was provided to generate result because all three algorithms to create ensemble were available and also similar to the other one."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.sac import MlpPolicy\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.a2c import MlpPolicy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3 import DDPG\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "\n"
   ]
  },
  {
   "source": [
    "Defining a function to train data over A2C algorithm"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_A2C(env_train, model_name, timesteps=25000):\n",
    "    \"\"\"A2C model\"\"\"\n",
    "\n",
    "    start = time.time()\n",
    "    model = A2C('MlpPolicy', env_train, verbose=0)\n",
    "    model.learn(total_timesteps=timesteps)\n",
    "    end = time.time()\n",
    "\n",
    "    model.save(f\"{TRAINED_MODEL_DIR}/{model_name}\")\n",
    "    print('Training time (A2C): ', (end - start) / 60, ' minutes')\n",
    "    return model"
   ]
  },
  {
   "source": [
    "Defining a function to train data over DDPG algorithm"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_DDPG(env_train, model_name, timesteps=10000):\n",
    "    \"\"\"DDPG model\"\"\"\n",
    "\n",
    "    # add the noise objects for DDPG\n",
    "    n_actions = env_train.action_space.shape[-1]\n",
    "    action_noise = OrnsteinUhlenbeckActionNoise(mean=np.zeros(n_actions), sigma=float(0.5) * np.ones(n_actions))\n",
    "\n",
    "    start = time.time()\n",
    "    model = DDPG('MlpPolicy', env_train, action_noise=action_noise)\n",
    "    model.learn(total_timesteps=timesteps)\n",
    "    end = time.time()\n",
    "\n",
    "    model.save(f\"{TRAINED_MODEL_DIR}/{model_name}\")\n",
    "    print('Training time (DDPG): ', (end-start)/60,' minutes')\n",
    "    return model"
   ]
  },
  {
   "source": [
    "Defining a function to train data over PPO algorithm"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_PPO(env_train, model_name, timesteps=50000):\n",
    "    \"\"\"PPO model\"\"\"\n",
    "\n",
    "    start = time.time()\n",
    "    model = PPO('MlpPolicy', env_train, ent_coef = 0.005, batch_size = 8)\n",
    "\n",
    "    model.learn(total_timesteps=timesteps)\n",
    "    end = time.time()\n",
    "\n",
    "    model.save(f\"{TRAINED_MODEL_DIR}/{model_name}\")\n",
    "    print('Training time (PPO): ', (end - start) / 60, ' minutes')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DRL_prediction(df,\n",
    "                   model,\n",
    "                   name,\n",
    "                   last_state,\n",
    "                   iter_num,\n",
    "                   unique_trade_date,\n",
    "                   rebalance_window,\n",
    "                   turbulence_threshold,\n",
    "                   initial):\n",
    "    ### make a prediction based on trained model###\n",
    "\n",
    "    ## trading env\n",
    "    trade_data = data_split(df, start=unique_trade_date[iter_num - rebalance_window], end=unique_trade_date[iter_num])\n",
    "    env_trade = DummyVecEnv([lambda: StockEnvTrade(trade_data,\n",
    "                                                   turbulence_threshold=turbulence_threshold,\n",
    "                                                   initial=initial,\n",
    "                                                   previous_state=last_state,\n",
    "                                                   model_name=name,\n",
    "                                                   iteration=iter_num)])\n",
    "    obs_trade = env_trade.reset()\n",
    "\n",
    "    for i in range(len(trade_data.index.unique())):\n",
    "        action, _states = model.predict(obs_trade)\n",
    "        obs_trade, rewards, dones, info = env_trade.step(action)\n",
    "        if i == (len(trade_data.index.unique()) - 2):\n",
    "            last_state = env_trade.render()\n",
    "\n",
    "    df_last_state = pd.DataFrame({'last_state': last_state})\n",
    "    df_last_state.to_csv('results/last_state_{}_{}.csv'.format(name, i), index=False)\n",
    "    return last_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DRL_validation(model, test_data, test_env, test_obs) -> None:\n",
    "    ###validation process###\n",
    "    for i in range(len(test_data.index.unique())):\n",
    "        action, _states = model.predict(test_obs)\n",
    "        test_obs, rewards, dones, info = test_env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_validation_sharpe(iteration):\n",
    "    ###Calculate Sharpe ratio based on validation results###\n",
    "    df_total_value = pd.read_csv('results/account_value_validation_{}.csv'.format(iteration), index_col=0)\n",
    "    df_total_value.columns = ['account_value_train']\n",
    "    df_total_value['daily_return'] = df_total_value.pct_change(1)\n",
    "    sharpe = (4 ** 0.5) * df_total_value['daily_return'].mean() / \\\n",
    "             df_total_value['daily_return'].std()\n",
    "    return sharpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ensemble_strategy(df, unique_trade_date, rebalance_window, validation_window) -> None:\n",
    "    \"\"\"Ensemble Strategy that combines PPO, A2C and DDPG\"\"\"\n",
    "    print(\"============Start Ensemble Strategy============\")\n",
    "    # for ensemble model, it's necessary to feed the last state\n",
    "    # of the previous model to the current model as the initial state\n",
    "    last_state_ensemble = []\n",
    "\n",
    "    ppo_sharpe_list = []\n",
    "    ddpg_sharpe_list = []\n",
    "    a2c_sharpe_list = []\n",
    "\n",
    "    model_use = []\n",
    "\n",
    "    # based on the analysis of the in-sample data\n",
    "    #turbulence_threshold = 140\n",
    "    insample_turbulence = df[(df.datadate<20151000) & (df.datadate>=20090000)]\n",
    "    insample_turbulence = insample_turbulence.drop_duplicates(subset=['datadate'])\n",
    "    insample_turbulence_threshold = np.quantile(insample_turbulence.turbulence.values, .90)\n",
    "\n",
    "    start = time.time()\n",
    "    for i in range(rebalance_window + validation_window, len(unique_trade_date), rebalance_window):\n",
    "        print(\"============================================\")\n",
    "        ## initial state is empty\n",
    "        if i - rebalance_window - validation_window == 0:\n",
    "            # inital state\n",
    "            initial = True\n",
    "        else:\n",
    "            # previous state\n",
    "            initial = False\n",
    "\n",
    "        # Tuning trubulence index based on historical data\n",
    "        # Turbulence lookback window is one quarter\n",
    "        end_date_index = df.index[df[\"datadate\"] == unique_trade_date[i - rebalance_window - validation_window]].to_list()[-1]\n",
    "        start_date_index = end_date_index - validation_window*30 + 1\n",
    "\n",
    "        historical_turbulence = df.iloc[start_date_index:(end_date_index + 1), :]\n",
    "        \n",
    "\n",
    "        historical_turbulence = historical_turbulence.drop_duplicates(subset=['datadate'])\n",
    "\n",
    "        historical_turbulence_mean = np.mean(historical_turbulence.turbulence.values)\n",
    "\n",
    "        if historical_turbulence_mean > insample_turbulence_threshold:\n",
    "            # if the mean of the historical data is greater than the 90% quantile of insample turbulence data\n",
    "            # then we assume that the current market is volatile,\n",
    "            # therefore we set the 90% quantile of insample turbulence data as the turbulence threshold\n",
    "            # meaning the current turbulence can't exceed the 90% quantile of insample turbulence data\n",
    "            turbulence_threshold = insample_turbulence_threshold\n",
    "        else:\n",
    "            # if the mean of the historical data is less than the 90% quantile of insample turbulence data\n",
    "            # then we tune up the turbulence_threshold, meaning we lower the risk\n",
    "            turbulence_threshold = np.quantile(insample_turbulence.turbulence.values, 1)\n",
    "        print(\"turbulence_threshold: \", turbulence_threshold)\n",
    "\n",
    "        ############## Environment Setup starts ##############\n",
    "        ## training env\n",
    "        train = data_split(df, start=20090000, end=unique_trade_date[i - rebalance_window - validation_window])\n",
    "        env_train = DummyVecEnv([lambda: StockEnvTrain(train)])\n",
    "\n",
    "        ## validation env\n",
    "        validation = data_split(df, start=unique_trade_date[i - rebalance_window - validation_window],\n",
    "                                end=unique_trade_date[i - rebalance_window])\n",
    "        env_val = DummyVecEnv([lambda: StockEnvValidation(validation,\n",
    "                                                          turbulence_threshold=turbulence_threshold,\n",
    "                                                          iteration=i)])\n",
    "        obs_val = env_val.reset()\n",
    "        ############## Environment Setup ends ##############\n",
    "\n",
    "        ############## Training and Validation starts ##############\n",
    "        print(\"======Model training from: \", 20090000, \"to \",\n",
    "              unique_trade_date[i - rebalance_window - validation_window])\n",
    "        \n",
    "        print(\"======A2C Training========\")\n",
    "        model_a2c = train_A2C(env_train, model_name=\"A2C_30k_dow_{}\".format(i), timesteps=30000)\n",
    "        print(\"======A2C Validation from: \", unique_trade_date[i - rebalance_window - validation_window], \"to \",\n",
    "              unique_trade_date[i - rebalance_window])\n",
    "        DRL_validation(model=model_a2c, test_data=validation, test_env=env_val, test_obs=obs_val)\n",
    "        sharpe_a2c = get_validation_sharpe(i)\n",
    "        print(\"A2C Sharpe Ratio: \", sharpe_a2c)\n",
    "\n",
    "        print(\"======PPO Training========\")\n",
    "        model_ppo = train_PPO(env_train, model_name=\"PPO_100k_dow_{}\".format(i), timesteps=100000)\n",
    "        print(\"======PPO Validation from: \", unique_trade_date[i - rebalance_window - validation_window], \"to \",\n",
    "              unique_trade_date[i - rebalance_window])\n",
    "        DRL_validation(model=model_ppo, test_data=validation, test_env=env_val, test_obs=obs_val)\n",
    "        sharpe_ppo = get_validation_sharpe(i)\n",
    "        print(\"PPO Sharpe Ratio: \", sharpe_ppo)\n",
    "\n",
    "        print(\"======DDPG Training========\")\n",
    "        model_ddpg = train_DDPG(env_train, model_name=\"DDPG_10k_dow_{}\".format(i), timesteps=10000)\n",
    "        #model_ddpg = train_TD3(env_train, model_name=\"DDPG_10k_dow_{}\".format(i), timesteps=20000)\n",
    "        print(\"======DDPG Validation from: \", unique_trade_date[i - rebalance_window - validation_window], \"to \",\n",
    "              unique_trade_date[i - rebalance_window])\n",
    "        DRL_validation(model=model_ddpg, test_data=validation, test_env=env_val, test_obs=obs_val)\n",
    "        sharpe_ddpg = get_validation_sharpe(i)\n",
    "\n",
    "        ppo_sharpe_list.append(sharpe_ppo)\n",
    "        a2c_sharpe_list.append(sharpe_a2c)\n",
    "        ddpg_sharpe_list.append(sharpe_ddpg)\n",
    "\n",
    "        # Model Selection based on sharpe ratio\n",
    "        if (sharpe_ppo >= sharpe_a2c) & (sharpe_ppo >= sharpe_ddpg):\n",
    "            model_ensemble = model_ppo\n",
    "            model_use.append('PPO')\n",
    "        elif (sharpe_a2c > sharpe_ppo) & (sharpe_a2c > sharpe_ddpg):\n",
    "            model_ensemble = model_a2c\n",
    "            model_use.append('A2C')\n",
    "        else:\n",
    "            model_ensemble = model_ddpg\n",
    "            model_use.append('DDPG')\n",
    "        ############## Training and Validation ends ##############\n",
    "\n",
    "        ############## Trading starts ##############\n",
    "        print(\"======Trading from: \", unique_trade_date[i - rebalance_window], \"to \", unique_trade_date[i])\n",
    "\n",
    "        last_state_ensemble = DRL_prediction(df=df, model=model_ensemble, name=\"ensemble\",\n",
    "                                             last_state=last_state_ensemble, iter_num=i,\n",
    "                                             unique_trade_date=unique_trade_date,\n",
    "                                             rebalance_window=rebalance_window,\n",
    "                                             turbulence_threshold=turbulence_threshold,\n",
    "                                             initial=initial)\n",
    "        # print(\"============Trading Done============\")\n",
    "        ############## Trading ends ##############\n",
    "\n",
    "    end = time.time()\n",
    "    print(\"Ensemble Strategy took: \", (end - start) / 60, \" minutes\")"
   ]
  },
  {
   "source": [
    "Generating Pre processing data to run ensemble stratergy"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   datadate   tic      adjcp       open       high        low      volume  \\\n0  20090102  AAPL  12.964286  12.268571  13.005714  12.165714  26641980.0   \n1  20090102   AXP  19.330000  18.570000  19.520000  18.400000  10955620.0   \n2  20090102    BA  45.250000  42.800000  45.560000  42.780000   7010171.0   \n3  20090102   CAT  46.910000  44.910000  46.980000  44.710000   7116726.0   \n4  20090102  CSCO  16.960000  16.410000  17.000000  16.250000  40977480.0   \n\n   macd    rsi        cci    adx  turbulence  \n0   0.0  100.0  66.666667  100.0         0.0  \n1   0.0  100.0  66.666667  100.0         0.0  \n2   0.0  100.0  66.666667  100.0         0.0  \n3   0.0    0.0  66.666667  100.0         0.0  \n4   0.0  100.0  66.666667  100.0         0.0  \n1053360\n"
     ]
    }
   ],
   "source": [
    "preprocessed_path = \"done_data.csv\"\n",
    "if os.path.exists(preprocessed_path):\n",
    "    data = pd.read_csv(preprocessed_path, index_col=0)\n",
    "else:\n",
    "    data = preprocess_data()\n",
    "    data = add_turbulence(data)\n",
    "    data.to_csv(preprocessed_path)\n",
    "\n",
    "print(data.head())\n",
    "print(data.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[20151002 20151005 20151006 ... 20200702 20200706 20200707]\n",
      "============Start Ensemble Strategy============\n",
      "============================================\n",
      "turbulence_threshold:  171.0940715631063\n",
      "======Model training from:  20090000 to  20151002\n",
      "======A2C Training========\n",
      "Training time (A2C):  1.8692237257957458  minutes\n",
      "======A2C Validation from:  20151002 to  20160104\n",
      "A2C Sharpe Ratio:  0.11173413062587269\n",
      "======PPO Training========\n",
      "Training time (PPO):  16.49744579792023  minutes\n",
      "======PPO Validation from:  20151002 to  20160104\n",
      "PPO Sharpe Ratio:  0.021519768529278567\n",
      "======DDPG Training========\n",
      "Training time (DDPG):  1.4921461502710978  minutes\n",
      "======DDPG Validation from:  20151002 to  20160104\n",
      "======Trading from:  20160104 to  20160405\n",
      "previous_total_asset:1000000\n",
      "end_total_asset:1097949.3867595755\n",
      "total_reward:97949.38675957546\n",
      "total_cost:  2093.6443957127362\n",
      "total trades:  1048\n",
      "Sharpe:  0.3089217392342952\n",
      "============================================\n",
      "turbulence_threshold:  96.08032158358247\n",
      "======Model training from:  20090000 to  20160104\n",
      "======A2C Training========\n",
      "Training time (A2C):  1.7740749716758728  minutes\n",
      "======A2C Validation from:  20160104 to  20160405\n",
      "A2C Sharpe Ratio:  0.17272304978948103\n",
      "======PPO Training========\n",
      "Training time (PPO):  16.70376450618108  minutes\n",
      "======PPO Validation from:  20160104 to  20160405\n",
      "PPO Sharpe Ratio:  0.18322813256359322\n",
      "======DDPG Training========\n",
      "Training time (DDPG):  1.578641140460968  minutes\n",
      "======DDPG Validation from:  20160104 to  20160405\n",
      "======Trading from:  20160405 to  20160705\n",
      "previous_total_asset:1097949.3867595755\n",
      "end_total_asset:1113223.8405494757\n",
      "total_reward:15274.45378990029\n",
      "total_cost:  952.6016756344769\n",
      "total trades:  669\n",
      "Sharpe:  0.059979298110863714\n",
      "============================================\n",
      "turbulence_threshold:  171.0940715631063\n",
      "======Model training from:  20090000 to  20160405\n",
      "======A2C Training========\n",
      "Training time (A2C):  1.8261796196301778  minutes\n",
      "======A2C Validation from:  20160405 to  20160705\n",
      "A2C Sharpe Ratio:  0.1755947799370544\n",
      "======PPO Training========\n",
      "Training time (PPO):  16.48028719027837  minutes\n",
      "======PPO Validation from:  20160405 to  20160705\n",
      "PPO Sharpe Ratio:  0.09877278693655207\n",
      "======DDPG Training========\n",
      "Training time (DDPG):  1.6079145073890686  minutes\n",
      "======DDPG Validation from:  20160405 to  20160705\n",
      "======Trading from:  20160705 to  20161003\n",
      "previous_total_asset:1113223.8405494757\n",
      "end_total_asset:1117895.2987141178\n",
      "total_reward:4671.458164642099\n",
      "total_cost:  3827.7640727330536\n",
      "total trades:  1305\n",
      "Sharpe:  0.026247057697796356\n",
      "============================================\n",
      "turbulence_threshold:  171.0940715631063\n",
      "======Model training from:  20090000 to  20160705\n",
      "======A2C Training========\n",
      "Training time (A2C):  1.9117974837621052  minutes\n",
      "======A2C Validation from:  20160705 to  20161003\n",
      "A2C Sharpe Ratio:  0.026163983697342883\n",
      "======PPO Training========\n",
      "Training time (PPO):  16.790583634376524  minutes\n",
      "======PPO Validation from:  20160705 to  20161003\n",
      "PPO Sharpe Ratio:  -0.054746084080847836\n",
      "======DDPG Training========\n",
      "Training time (DDPG):  1.5708829164505005  minutes\n",
      "======DDPG Validation from:  20160705 to  20161003\n",
      "======Trading from:  20161003 to  20170103\n",
      "previous_total_asset:1117895.2987141178\n",
      "end_total_asset:1185302.048136511\n",
      "total_reward:67406.74942239304\n",
      "total_cost:  859.2992310956863\n",
      "total trades:  1115\n",
      "Sharpe:  0.2815471943869732\n",
      "============================================\n",
      "turbulence_threshold:  171.0940715631063\n",
      "======Model training from:  20090000 to  20161003\n",
      "======A2C Training========\n",
      "Training time (A2C):  1.7367291927337647  minutes\n",
      "======A2C Validation from:  20161003 to  20170103\n",
      "A2C Sharpe Ratio:  0.5054174156366267\n",
      "======PPO Training========\n",
      "Training time (PPO):  16.01474586327871  minutes\n",
      "======PPO Validation from:  20161003 to  20170103\n",
      "PPO Sharpe Ratio:  0.41222577687354217\n",
      "======DDPG Training========\n",
      "Training time (DDPG):  1.7435332576433817  minutes\n",
      "======DDPG Validation from:  20161003 to  20170103\n",
      "======Trading from:  20170103 to  20170404\n",
      "previous_total_asset:1185302.048136511\n",
      "end_total_asset:1290715.3145784542\n",
      "total_reward:105413.26644194336\n",
      "total_cost:  2344.568765808711\n",
      "total trades:  1262\n",
      "Sharpe:  0.5739349600075424\n",
      "============================================\n",
      "turbulence_threshold:  96.08032158358247\n",
      "======Model training from:  20090000 to  20170103\n",
      "======A2C Training========\n",
      "Training time (A2C):  1.8228418231010437  minutes\n",
      "======A2C Validation from:  20170103 to  20170404\n",
      "A2C Sharpe Ratio:  0.0069008613929133185\n",
      "======PPO Training========\n",
      "Training time (PPO):  16.025750784079232  minutes\n",
      "======PPO Validation from:  20170103 to  20170404\n",
      "PPO Sharpe Ratio:  0.4307651223947282\n",
      "======DDPG Training========\n",
      "Training time (DDPG):  1.5105864842732748  minutes\n",
      "======DDPG Validation from:  20170103 to  20170404\n",
      "======Trading from:  20170404 to  20170705\n",
      "previous_total_asset:1290715.3145784542\n",
      "end_total_asset:1329158.3540104865\n",
      "total_reward:38443.03943203227\n",
      "total_cost:  7609.838341914272\n",
      "total trades:  1322\n",
      "Sharpe:  0.2754968651909691\n",
      "============================================\n",
      "turbulence_threshold:  171.0940715631063\n",
      "======Model training from:  20090000 to  20170404\n",
      "======A2C Training========\n",
      "Training time (A2C):  1.8155309836069742  minutes\n",
      "======A2C Validation from:  20170404 to  20170705\n",
      "A2C Sharpe Ratio:  0.39900076290043796\n",
      "======PPO Training========\n",
      "Training time (PPO):  16.553467384974162  minutes\n",
      "======PPO Validation from:  20170404 to  20170705\n",
      "PPO Sharpe Ratio:  0.1044242773538049\n",
      "======DDPG Training========\n",
      "Training time (DDPG):  1.5468903183937073  minutes\n",
      "======DDPG Validation from:  20170404 to  20170705\n",
      "======Trading from:  20170705 to  20171003\n",
      "previous_total_asset:1329158.3540104865\n",
      "end_total_asset:1340884.0999270517\n",
      "total_reward:11725.745916565182\n",
      "total_cost:  2499.2544512773375\n",
      "total trades:  910\n",
      "Sharpe:  0.07371948188311743\n",
      "============================================\n",
      "turbulence_threshold:  171.0940715631063\n",
      "======Model training from:  20090000 to  20170705\n",
      "======A2C Training========\n",
      "Training time (A2C):  1.796553099155426  minutes\n",
      "======A2C Validation from:  20170705 to  20171003\n",
      "A2C Sharpe Ratio:  -0.005421283835639088\n",
      "======PPO Training========\n",
      "Training time (PPO):  16.468138813972473  minutes\n",
      "======PPO Validation from:  20170705 to  20171003\n",
      "PPO Sharpe Ratio:  0.2967568748665434\n",
      "======DDPG Training========\n",
      "Training time (DDPG):  1.4814094265302022  minutes\n",
      "======DDPG Validation from:  20170705 to  20171003\n",
      "======Trading from:  20171003 to  20180103\n",
      "previous_total_asset:1340884.0999270517\n",
      "end_total_asset:1464983.1371670517\n",
      "total_reward:124099.03723999998\n",
      "total_cost:  977.3177599999998\n",
      "total trades:  920\n",
      "Sharpe:  0.6438889424246339\n",
      "============================================\n",
      "turbulence_threshold:  96.08032158358247\n",
      "======Model training from:  20090000 to  20171003\n",
      "======A2C Training========\n",
      "Training time (A2C):  1.956122386455536  minutes\n",
      "======A2C Validation from:  20171003 to  20180103\n",
      "A2C Sharpe Ratio:  0.5043102379491529\n",
      "======PPO Training========\n",
      "Training time (PPO):  16.225980333487193  minutes\n",
      "======PPO Validation from:  20171003 to  20180103\n",
      "PPO Sharpe Ratio:  0.4519802239233434\n",
      "======DDPG Training========\n",
      "Training time (DDPG):  1.5835866411526998  minutes\n",
      "======DDPG Validation from:  20171003 to  20180103\n",
      "======Trading from:  20180103 to  20180405\n",
      "previous_total_asset:1464983.1371670517\n",
      "end_total_asset:1479181.3369881078\n",
      "total_reward:14198.1998210561\n",
      "total_cost:  2335.3629926695753\n",
      "total trades:  341\n",
      "Sharpe:  0.07304636730590225\n",
      "============================================\n",
      "turbulence_threshold:  96.08032158358247\n",
      "======Model training from:  20090000 to  20180103\n",
      "======A2C Training========\n",
      "Training time (A2C):  1.9346710244814556  minutes\n",
      "======A2C Validation from:  20180103 to  20180405\n",
      "A2C Sharpe Ratio:  0.010068430243695949\n",
      "======PPO Training========\n",
      "Training time (PPO):  16.219836235046387  minutes\n",
      "======PPO Validation from:  20180103 to  20180405\n",
      "PPO Sharpe Ratio:  -0.025475141525801714\n",
      "======DDPG Training========\n",
      "Training time (DDPG):  1.6557029604911804  minutes\n",
      "======DDPG Validation from:  20180103 to  20180405\n",
      "======Trading from:  20180405 to  20180705\n",
      "previous_total_asset:1479181.3369881078\n",
      "end_total_asset:1482086.0012347866\n",
      "total_reward:2904.664246678818\n",
      "total_cost:  4974.448638450952\n",
      "total trades:  910\n",
      "Sharpe:  0.01655480840221972\n",
      "============================================\n",
      "turbulence_threshold:  96.08032158358247\n",
      "======Model training from:  20090000 to  20180405\n",
      "======A2C Training========\n",
      "Training time (A2C):  1.949485143025716  minutes\n",
      "======A2C Validation from:  20180405 to  20180705\n",
      "A2C Sharpe Ratio:  -0.042584928994729265\n",
      "======PPO Training========\n",
      "Training time (PPO):  16.25210886001587  minutes\n",
      "======PPO Validation from:  20180405 to  20180705\n",
      "PPO Sharpe Ratio:  -0.09804189726922315\n",
      "======DDPG Training========\n",
      "Training time (DDPG):  1.688191004594167  minutes\n",
      "======DDPG Validation from:  20180405 to  20180705\n",
      "======Trading from:  20180705 to  20181003\n",
      "previous_total_asset:1482086.0012347866\n",
      "end_total_asset:1489109.401486877\n",
      "total_reward:7023.400252090301\n",
      "total_cost:  6479.896239690595\n",
      "total trades:  980\n",
      "Sharpe:  0.05473412909988272\n",
      "============================================\n",
      "turbulence_threshold:  96.08032158358247\n",
      "======Model training from:  20090000 to  20180705\n",
      "======A2C Training========\n",
      "Training time (A2C):  1.948050586382548  minutes\n",
      "======A2C Validation from:  20180705 to  20181003\n",
      "A2C Sharpe Ratio:  0.18552133104851537\n",
      "======PPO Training========\n",
      "Training time (PPO):  16.203561226526897  minutes\n",
      "======PPO Validation from:  20180705 to  20181003\n",
      "PPO Sharpe Ratio:  0.2236482776514361\n",
      "======DDPG Training========\n",
      "Training time (DDPG):  1.7398016889890036  minutes\n",
      "======DDPG Validation from:  20180705 to  20181003\n",
      "======Trading from:  20181003 to  20190104\n",
      "previous_total_asset:1489109.401486877\n",
      "end_total_asset:1500718.8111614399\n",
      "total_reward:11609.40967456298\n",
      "total_cost:  931.643806926424\n",
      "total trades:  178\n",
      "Sharpe:  0.2974837756275\n",
      "============================================\n",
      "turbulence_threshold:  171.0940715631063\n",
      "======Model training from:  20090000 to  20181003\n",
      "======A2C Training========\n",
      "Training time (A2C):  1.945016054312388  minutes\n",
      "======A2C Validation from:  20181003 to  20190104\n",
      "A2C Sharpe Ratio:  -0.39532566702187655\n",
      "======PPO Training========\n",
      "Training time (PPO):  16.214337960879007  minutes\n",
      "======PPO Validation from:  20181003 to  20190104\n",
      "PPO Sharpe Ratio:  -0.4210823198827311\n",
      "======DDPG Training========\n",
      "Training time (DDPG):  1.788050321737925  minutes\n",
      "======DDPG Validation from:  20181003 to  20190104\n",
      "======Trading from:  20190104 to  20190405\n",
      "previous_total_asset:1500718.8111614399\n",
      "end_total_asset:1621162.779601932\n",
      "total_reward:120443.96844049217\n",
      "total_cost:  1552.485950186019\n",
      "total trades:  989\n",
      "Sharpe:  0.33373827539078765\n",
      "============================================\n",
      "turbulence_threshold:  96.08032158358247\n",
      "======Model training from:  20090000 to  20190104\n",
      "======A2C Training========\n",
      "Training time (A2C):  1.9751494208971658  minutes\n",
      "======A2C Validation from:  20190104 to  20190405\n",
      "A2C Sharpe Ratio:  0.17114594839473732\n",
      "======PPO Training========\n",
      "Training time (PPO):  16.239584950606027  minutes\n",
      "======PPO Validation from:  20190104 to  20190405\n",
      "PPO Sharpe Ratio:  0.13601852589456392\n",
      "======DDPG Training========\n",
      "Training time (DDPG):  1.4656109054883322  minutes\n",
      "======DDPG Validation from:  20190104 to  20190405\n",
      "======Trading from:  20190405 to  20190708\n",
      "previous_total_asset:1621162.779601932\n",
      "end_total_asset:1627176.407467522\n",
      "total_reward:6013.627865589922\n",
      "total_cost:  1510.5812843358603\n",
      "total trades:  166\n",
      "Sharpe:  0.22561124378913414\n",
      "============================================\n",
      "turbulence_threshold:  96.08032158358247\n",
      "======Model training from:  20090000 to  20190405\n",
      "======A2C Training========\n",
      "Training time (A2C):  1.979986596107483  minutes\n",
      "======A2C Validation from:  20190405 to  20190708\n",
      "A2C Sharpe Ratio:  0.1930454817028311\n",
      "======PPO Training========\n",
      "Training time (PPO):  16.29160602490107  minutes\n",
      "======PPO Validation from:  20190405 to  20190708\n",
      "PPO Sharpe Ratio:  0.25249557942488754\n",
      "======DDPG Training========\n",
      "Training time (DDPG):  1.5011715531349181  minutes\n",
      "======DDPG Validation from:  20190405 to  20190708\n",
      "======Trading from:  20190708 to  20191004\n",
      "previous_total_asset:1627176.407467522\n",
      "end_total_asset:1631068.351470019\n",
      "total_reward:3891.94400249701\n",
      "total_cost:  2855.6593323627217\n",
      "total trades:  267\n",
      "Sharpe:  0.04681867575994515\n",
      "============================================\n",
      "turbulence_threshold:  96.08032158358247\n",
      "======Model training from:  20090000 to  20190708\n",
      "======A2C Training========\n",
      "Training time (A2C):  1.985662305355072  minutes\n",
      "======A2C Validation from:  20190708 to  20191004\n",
      "A2C Sharpe Ratio:  0.029798758791485486\n",
      "======PPO Training========\n",
      "Training time (PPO):  16.29838775396347  minutes\n",
      "======PPO Validation from:  20190708 to  20191004\n",
      "PPO Sharpe Ratio:  -0.18859033684722393\n",
      "======DDPG Training========\n",
      "Training time (DDPG):  1.546367081006368  minutes\n",
      "======DDPG Validation from:  20190708 to  20191004\n",
      "======Trading from:  20191004 to  20200106\n",
      "previous_total_asset:1631068.351470019\n",
      "end_total_asset:1630554.9434700191\n",
      "total_reward:-513.4079999998212\n",
      "total_cost:  301.65899999999993\n",
      "total trades:  48\n",
      "Sharpe:  -0.12149693004235755\n",
      "============================================\n",
      "turbulence_threshold:  96.08032158358247\n",
      "======Model training from:  20090000 to  20191004\n",
      "======A2C Training========\n",
      "Training time (A2C):  1.9884622017542521  minutes\n",
      "======A2C Validation from:  20191004 to  20200106\n",
      "A2C Sharpe Ratio:  -0.18539558017693\n",
      "======PPO Training========\n",
      "Training time (PPO):  16.397002780437468  minutes\n",
      "======PPO Validation from:  20191004 to  20200106\n",
      "PPO Sharpe Ratio:  -0.3412367463012458\n",
      "======DDPG Training========\n",
      "Training time (DDPG):  1.583048689365387  minutes\n",
      "======DDPG Validation from:  20191004 to  20200106\n",
      "======Trading from:  20200106 to  20200406\n",
      "previous_total_asset:1630554.9434700191\n",
      "end_total_asset:1614485.413728949\n",
      "total_reward:-16069.529741070233\n",
      "total_cost:  907.8903682870764\n",
      "total trades:  142\n",
      "Sharpe:  -0.41945346191908134\n",
      "============================================\n",
      "turbulence_threshold:  96.08032158358247\n",
      "======Model training from:  20090000 to  20200106\n",
      "======A2C Training========\n",
      "Training time (A2C):  2.0069274584452312  minutes\n",
      "======A2C Validation from:  20200106 to  20200406\n",
      "A2C Sharpe Ratio:  -0.4181220864583912\n",
      "======PPO Training========\n",
      "Training time (PPO):  16.375236948331196  minutes\n",
      "======PPO Validation from:  20200106 to  20200406\n",
      "PPO Sharpe Ratio:  -0.45078520931324795\n",
      "======DDPG Training========\n",
      "Training time (DDPG):  1.622320008277893  minutes\n",
      "======DDPG Validation from:  20200106 to  20200406\n",
      "======Trading from:  20200406 to  20200707\n",
      "previous_total_asset:1614485.413728949\n",
      "end_total_asset:1617131.0852562787\n",
      "total_reward:2645.6715273298323\n",
      "total_cost:  559.9590914394857\n",
      "total trades:  121\n",
      "Sharpe:  0.16803216984912012\n",
      "Ensemble Strategy took:  357.4124521970749  minutes\n"
     ]
    }
   ],
   "source": [
    "unique_trade_date = data[(data.datadate > 20151001)&(data.datadate <= 20200707)].datadate.unique()\n",
    "print(unique_trade_date)\n",
    "\n",
    "rebalance_window = 63\n",
    "validation_window = 63\n",
    "    \n",
    "## Ensemble Strategy\n",
    "run_ensemble_strategy(df=data, \n",
    "                          unique_trade_date= unique_trade_date,\n",
    "                          rebalance_window = rebalance_window,\n",
    "                          validation_window=validation_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}